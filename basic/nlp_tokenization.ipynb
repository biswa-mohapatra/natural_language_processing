{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization means splitting text into meaningful unit words. There are sentence tokenizers as well as word tokenizers.\n",
    "\n",
    "Sentence tokenizer splits a paragraph into meaningful sentences, while word tokenizer splits a sentence into unit meaningful words. Many libraries can perform tokenization like SpaCy, NLTK, and TextBlob.\n",
    "\n",
    "Splitting a sentence on space to get individual unit words can be understood as tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Tokenization means splitting text into meaningful unit words. There are sentence tokenizers as well as word tokenizers.\n",
    "\n",
    "Sentence tokenizer splits a paragraph into meaningful sentences, while word tokenizer splits a sentence into unit meaningful words. Many libraries can perform tokenization like SpaCy, NLTK, and TextBlob.\n",
    "\n",
    "Splitting a sentence on space to get individual unit words can be understood as tokenization.\"\"\"\n",
    "\n",
    "sentence = \"Tokenization means splitting text into meaningful unit words.\"\n",
    "\n",
    "word = \"Tokenization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization means splitting text into meaningful unit words', ' There are sentence tokenizers as well as word tokenizers', '\\n\\nSentence tokenizer splits a paragraph into meaningful sentences, while word tokenizer splits a sentence into unit meaningful words', ' Many libraries can perform tokenization like SpaCy, NLTK, and TextBlob', '\\n\\nSplitting a sentence on space to get individual unit words can be understood as tokenization', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sent_tokenization(paragraph):\n",
    "    return paragraph.split('.')\n",
    "\n",
    "print(sent_tokenization(paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization means splitting text into meaningful unit words.', 'There are sentence tokenizers as well as word tokenizers.', 'Sentence tokenizer splits a paragraph into meaningful sentences, while word tokenizer splits a sentence into unit meaningful words.', 'Many libraries can perform tokenization like SpaCy, NLTK, and TextBlob.', 'Splitting a sentence on space to get individual unit words can be understood as tokenization.']\n"
     ]
    }
   ],
   "source": [
    "# using library\n",
    "import nltk\n",
    "tokens = nltk.sent_tokenize(paragraph)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'means', 'splitting', 'text', 'into', 'meaningful', 'unit', 'words.']\n"
     ]
    }
   ],
   "source": [
    "def word_tokenize(sentence):\n",
    "    return sentence.split(\" \")\n",
    "\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'means', 'splitting', 'text', 'into', 'meaningful', 'unit', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "# with library\n",
    "print(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### character level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n']\n"
     ]
    }
   ],
   "source": [
    "def char_tokenization(word):\n",
    "    return [char for char in word]\n",
    "\n",
    "print(char_tokenization(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
